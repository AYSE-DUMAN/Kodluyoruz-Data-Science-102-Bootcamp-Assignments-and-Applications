{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST VERİ SETİ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "MNIST, her biri 28 x 28 piksellik, 0 ila 255 piksel değerlerine sahip gri tonlamalı 70.000 elle yazılmış rakam görüntüsü içerir. Verileri kendimiz indirebilir ve önceden işleyebiliriz. Ancak scikit-learn'ün içinde yer alan veri setlerinde bizim istediğimiz mnist veri seti bulunmaktadır.\n",
    "Bu yüzden bu veri setini sklearn kütüphanesinden indirmeye karar verdim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n"
     ]
    }
   ],
   "source": [
    "print(mnist_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist veri setindeki target ve data keylerini alarak veri setini hazırlayalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (70000, 784) Shape of y: (70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = mnist_data['data'], mnist_data['target']\n",
    "\n",
    "print('Shape of X:', X.shape, 'Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradyanlarımızı daha kolay tutmak için verileri normalleştireceğiz:\n",
    "\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode labels\n",
    "# Burada 10 x 70.000'lik bir dizi elde etmek için MNIST etiketlerini kodlayacağız\n",
    "\n",
    "digits = 10  # veri setindeki farklı rakam adedi\n",
    "examples = y.shape[0]  # 70000 örneklem sayımız mevcut\n",
    "\n",
    "y = y.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[y.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "print(Y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ve test datası için gerekli splitleri yapalım\n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (784, 60000) Shape of Y_train: (10, 60000)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train:', X_train.shape, 'Shape of Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (784, 10000) Shape of Y_test: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_test:', X_test.shape, 'Shape of Y_test:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 101\n",
    "\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "Y_train[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çıktıda görüldüğü gibi arraydeki sırası 7.konumda bulunmaktadır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 302\n",
    "\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "Y_train[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çıktıda görüldüğü gibi arraydeki sırası 5.konumda bulunmaktadır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function Tanımlanması (Çoklu Sınıf İçin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Maliyet fonksiyonumuz ikiden fazla sınıfa genellemek yaptığı için N sınıflı genel formülü aşağıda verilmiştir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $ L(y, \\hat{y}) = -\\sum_{i = 0}^n y_i \\log(\\hat{y}_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ L(Y, \\hat{Y}) = - \\frac{1}{m} \\sum_{j = 0}^m \\sum_{i = 0}^n y_i^{(j)} \\log(\\hat{y}_i^{(j)}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiclass_loss(Y, Y_hat):\n",
    "\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "    \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, params):\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(X, Y, params, cache):\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(138)\n",
    "\n",
    "# hyperparameters\n",
    "n_x = X_train.shape[0]\n",
    "n_h = 64\n",
    "learning_rate = 4\n",
    "beta = .9\n",
    "batch_size = 128\n",
    "batches = -(-m // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "\n",
    "\n",
    "params = { \"W1\": np.random.randn(n_h, n_x) * np.sqrt(1. / n_x),\n",
    "           \"b1\": np.zeros((n_h, 1)) * np.sqrt(1. / n_x),\n",
    "           \"W2\": np.random.randn(digits, n_h) * np.sqrt(1. / n_h),\n",
    "           \"b2\": np.zeros((digits, 1)) * np.sqrt(1. / n_h) }\n",
    "\n",
    "V_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "V_db1 = np.zeros(params[\"b1\"].shape)\n",
    "V_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "V_db2 = np.zeros(params[\"b2\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_dW1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_db1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_dW2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_db2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training cost = 0.10174268970240367, test cost = 0.11955876068494611\n",
      "Epoch 2: training cost = 0.07859889330831527, test cost = 0.10582850450366496\n",
      "Epoch 3: training cost = 0.061457215122181004, test cost = 0.09549396026058612\n",
      "Epoch 4: training cost = 0.05139485914441815, test cost = 0.08925562832226555\n",
      "Epoch 5: training cost = 0.050055535349367686, test cost = 0.09033835340539813\n",
      "Epoch 6: training cost = 0.039432130677260174, test cost = 0.08484458161601327\n",
      "Epoch 7: training cost = 0.03480206287011367, test cost = 0.0882855328896349\n",
      "Epoch 8: training cost = 0.041661985767138726, test cost = 0.10423563507976565\n",
      "Epoch 9: training cost = 0.02199897142568331, test cost = 0.08262176300615293\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "# mini-batch gradient descent oluşturalım\n",
    "\n",
    "for i in range(9):\n",
    "\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        feed = feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, feed)\n",
    "\n",
    "        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[\"dW1\"])\n",
    "        V_db1 = (beta * V_db1 + (1. - beta) * grads[\"db1\"])\n",
    "        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[\"dW2\"])\n",
    "        V_db2 = (beta * V_db2 + (1. - beta) * grads[\"db2\"])\n",
    "\n",
    "        params[\"W1\"] = params[\"W1\"] - learning_rate * V_dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - learning_rate * V_db1\n",
    "        params[\"W2\"] = params[\"W2\"] - learning_rate * V_dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - learning_rate * V_db2\n",
    "\n",
    "    feed = feed_forward(X_train, params)\n",
    "    train_cost = compute_multiclass_loss(Y_train, feed[\"A2\"])\n",
    "    feed = feed_forward(X_test, params)\n",
    "    test_cost = compute_multiclass_loss(Y_test, feed[\"A2\"])\n",
    "    print(\"Epoch {}: training cost = {}, test cost = {}\".format(i+1 ,train_cost, test_cost))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST İLE BINARY CLASSIFICATION OLUŞTURMAK İÇİN MODEL OLUŞTURALIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varsayılan MNIST etiketleri, yedi görüntüsü için 7'yi, dört görüntüsü için 4'ü vb. olarak kaydeder. Ancak şimdilik sadece bir sıfır sınıflandırıcı oluşturalım. Bu yüzden etiketlerimizin sıfır olduğunda 1, aksi takdirde 0 demesini istiyoruz . Bu nedenle, bunun gerçekleşmesi için etiketlerin üzerine yazacağız ve ona uygun cost function oluşturacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (70000, 784) \n",
      " Shape of y: (70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = mnist_data['data'], mnist_data['target']\n",
    "print('Shape of X:', X.shape,'\\n', 'Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradyanlarımızı yönetilebilir tutmak için verileri normalleştireceğiz:\n",
    "\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = np.zeros(y.shape)\n",
    "y_new[np.where(y == 0.0)[0]] = 1\n",
    "y = y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 60000\n",
    "m_test = X.shape[0] - m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:m].T, X[m:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 60000), (784, 10000))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = y[:m].reshape(1,m), y[m:].reshape(1,m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 60000), (1, 10000))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34046,  2951,  5692, ..., 14147, 56088, 38408])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "shuffle_index = np.random.permutation(m)\n",
    "shuffle_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi eğitim setinde oluşturduğumuz rastgele bir resme ve etiketine bir göz atalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFFUlEQVR4nO3dUVJTWRRA0ZcuZ6HOA3QagtNQcRiCmYYwD3AeOo/0V390VXKvnRCyadb65BQhfuy6VZ667602m80C9Px16i8AbCdOiBInRIkTosQJUa8mc/+VC8e32vZDJydEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IerVqb/AS/Tw8LDXbFmWZb1eD+e/fv0azs/Ozobzm5ubnbPz8/Ph7/K4nJwQJU6IEidEiROixAlR4oQocULUarPZjObDIdvNdpXv3r17om/yuH78+DGcX15ePtE3+d9ZbfuhkxOixAlR4oQocUKUOCFKnBBllXIEs6tVP3/+3Puz3759O5zProTN/vboytnss2crJHaySoHnRJwQJU6IEidEiROixAlR4oQoe84jWK22rq3+yMXFxXB+fX09nM/2oLNHZ75//37v372/vx/OPVpzJ3tOeE7ECVHihChxQpQ4IUqcECVOiPIKwD3c3t4e7bMP3WPOHHIfdLbnnM3tOf8bJydEiROixAlR4oQocUKUOCFKnBBlzxkze/broXvO2S7y7u5u788+9Lvxb05OiBInRIkTosQJUeKEKHFClDghyp5zD7N7iYe8I/Pr168H/e3ZrvGQd2jO/l3uaz4uJydEiROixAlR4oQocUKUOCHKKmUPh64rRiuH0ZplWcav6FuWZfn06dNwvl6vh/OR2esJeVxOTogSJ0SJE6LECVHihChxQpQ4IWq12WxG8+GQ/Yz2oB8/fhz+7uzRlocaXQs75LoZQ6ttP3RyQpQ4IUqcECVOiBInRIkTosQJUe5znsDoPufr16+Hv3vsPefNzc1RP58/5+SEKHFClDghSpwQJU6IEidEiROi7DlP4PLycuds9tzaY/v+/fvOmVf8PS0nJ0SJE6LECVHihChxQpQ4IUqcEOW5tUcwuxN5dXW1czZ79+e3b9/2+k7/mD0Xd+T6+no4//Lly96f/cJ5bi08J+KEKHFClDghSpwQJU6IcmVsD7PHU67X670/+8OHD8P56LrZn/j9+/dwPlrznPo620vj5IQocUKUOCFKnBAlTogSJ0SJE6LsOffw8PAwnM/2oGdnZztnx34F3yGPt7y7u3vEb8KMkxOixAlR4oQocUKUOCFKnBAlToiy5zyB2eMvj+nNmzdH++zb29vh/NC7qC+NkxOixAlR4oQocUKUOCFKnBAlToiy53xhZs+tPYQ95uNyckKUOCFKnBAlTogSJ0SJE6KsUk5g9Cq92WM1D71u5vGWz4eTE6LECVHihChxQpQ4IUqcECVOiFptNpvRfDhku9nVqUN2jRcXF8P5bE862rHOjF5duCzzVyOy02rbD52cECVOiBInRIkTosQJUeKEKHFClD3nEcx2jVdXVztn5fuW9/f3w/n5+fkTfZP/HXtOeE7ECVHihChxQpQ4IUqcECVOiLLnPIHRHnS25zzkPubsby/Lsnz+/HnnzCv+jsaeE54TcUKUOCFKnBAlTogSJ0SJE6LsOeH07DnhOREnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtSryXzrI/uA43NyQpQ4IUqcECVOiBInRIkTov4GOVbd3DXafPUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 7\n",
    "plt.imshow(X_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(y_train[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Çıktıdaki sayının 8 olduğu gözükmektedir.Bu nedenle etiketin 0 olması istediğimiz bir durumdur. Çünkü çıktısı 0 hariç tüm rakamlardaki etiketin 0 olmasını bekliyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Propogation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid fonksiyonunu oluşturalım"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\hat{y} = \\sigma(w^T x + b) $\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    result = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Tanımlayalım (İkili Sınıflandırma İçin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maliyet fonksiyonumuz için çapraz entropi kullanacağız. Tek bir eğitim örneğinin formülü aşağıdaki gibidir:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bir eğitim setindeki tüm  örneklerin ortalamasını alırsak aşağıdaki ifadeyi elde ederiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(Y, \\hat{Y}) = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)}) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute_loss fonksiyonunu oluşturalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, Y_hat):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    L = -(1/m) * (np.sum (np.multiply(np.log(Y_hat),Y)) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y))))\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \\begin{align}\n",
    "  z &= w^T x + b,\\newline\n",
    "  \\hat{y} &= \\sigma(z),\\newline\n",
    "  L(y, \\hat{y}) &= -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y}).\n",
    "  \\end{align}\n",
    "  \n",
    "  \n",
    "Geri yayılım için, L'nin w'nin her bir bileşeni wj'ye göre nasıl değiştiğini bilmemiz gerekmktedir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cost:  0.7115346719448323\n",
      "Epoch 200 cost:  1.2982816577870062e-05\n",
      "Epoch 400 cost:  1.265735571255418e-05\n",
      "Epoch 600 cost:  1.234903310001593e-05\n",
      "Epoch 800 cost:  1.2056494428385626e-05\n",
      "Epoch 1000 cost:  1.1778526201107384e-05\n",
      "Epoch 1200 cost:  1.1514037769655972e-05\n",
      "Epoch 1400 cost:  1.1262046061045431e-05\n",
      "Epoch 1600 cost:  1.1021662538679437e-05\n",
      "Epoch 1800 cost:  1.0792082023433848e-05\n",
      "Final cost: 1.0573646690375663e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "\n",
    "X = X_train\n",
    "Y = y_train\n",
    "\n",
    "n_x = X.shape[0] # 70 000 örneklem\n",
    "m = X.shape[1]  # 784 feature \n",
    "\n",
    "W = np.random.randn(n_x, 1) * 0.01\n",
    "b = np.zeros((1, 1))\n",
    "\n",
    "for i in range(2000):\n",
    "    \n",
    "    Z = np.matmul(W.T, X) + b\n",
    "    Y_head = sigmoid(Z)\n",
    "\n",
    "    cost = compute_loss(Y, Y_head)\n",
    "\n",
    "    dW = (1/m) * np.matmul(X, (Y_head-Y).T)\n",
    "    db = (1/m) * np.sum(Y_head-Y, axis=1, keepdims=True)\n",
    "\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    if (i % 200 == 0):\n",
    "        print(\"Epoch\", i, \"cost: \", cost)\n",
    "\n",
    "print(\"Final cost:\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŞİMDİ MNIST VERİ SETİNİ İLE PYTORCH KÜTÜPHANESİNDEN FAYDALANARAK MODEL OLUŞTURALIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from torch import optim\n",
    "import math\n",
    "from torchvision import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0,), (1,)),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = datasets.MNIST('.', download=False, train=True, transform=transform)\n",
    "testset = datasets.MNIST('.', download=False, train=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training  Validation\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 150\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training data): 48000\n",
      "(Validation data): 12000\n",
      "(Test data): 10000\n",
      "Batch : 320\n"
     ]
    }
   ],
   "source": [
    "batch_count = len(trainloader)\n",
    "print('(Training data):', len(train_idx))\n",
    "print('(Validation data):', len(valid_idx))\n",
    "print('(Test data):', len(testset))\n",
    "print('Batch :', batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predicted, target):\n",
    "    equals = predicted.max(1)[1] == target\n",
    "    return torch.mean(equals.type(torch.FloatTensor)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, epoch, save_file_name='model.pt'):\n",
    "    train_losses, valid_losses = [], []\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    for e in range(epoch):\n",
    "        batch_n = 1\n",
    "        batch_mean_loss = 0\n",
    "        batch_mean_accuracy = 0\n",
    "        print(f'Epoch: {e+1}/{epoch}')\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_mean_loss += loss.item()\n",
    "            batch_mean_accuracy += get_accuracy(log_ps, labels)\n",
    "\n",
    "            batch_n += 1\n",
    "        else:\n",
    "            valid_loss = 0\n",
    "            valid_accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for images, labels in validloader:\n",
    "                    log_ps = model(images)\n",
    "                    valid_loss += criterion(log_ps, labels)\n",
    "\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    valid_accuracy += get_accuracy(ps, labels)\n",
    "            model.train()\n",
    "            \n",
    "            valid_loss /= len(validloader)\n",
    "            valid_accuracy /= len(validloader)\n",
    "            \n",
    "            train_losses.append(batch_mean_loss/len(trainloader))\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            print(\"Training Loss: {:.3f}|| \".format(batch_mean_loss/len(trainloader)),\n",
    "                  \"Validation Loss: {:.3f}||\".format(valid_loss),\n",
    "                  \"Validation Accuracy: {:.3f}%\".format(valid_accuracy*100))\n",
    "            \n",
    "            if valid_loss_min > valid_loss:\n",
    "                print('\\tValidation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\\n'.format(valid_loss_min,valid_loss))\n",
    "                torch.save(model.state_dict(), save_file_name)\n",
    "                valid_loss_min = valid_loss\n",
    "    return model, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, labels in testloader:\n",
    "            log_ps = model(images)\n",
    "            test_loss += criterion(log_ps, labels)\n",
    "\n",
    "            ps = torch.exp(log_ps)\n",
    "            test_accuracy += get_accuracy(ps, labels)\n",
    "    model.train()\n",
    "\n",
    "    test_loss /= len(testloader)\n",
    "    test_accuracy /= len(testloader)\n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 160\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, output_size),\n",
    "                                    nn.LogSoftmax(dim=1))\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.07, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Training Loss: 0.164||  Validation Loss: 0.133|| Validation Accuracy: 96.025%\n",
      "\tValidation loss decreased (inf --> 0.132786).  Saving model ...\n",
      "\n",
      "Epoch: 2/10\n",
      "Training Loss: 0.112||  Validation Loss: 0.100|| Validation Accuracy: 96.967%\n",
      "\tValidation loss decreased (0.132786 --> 0.100182).  Saving model ...\n",
      "\n",
      "Epoch: 3/10\n",
      "Training Loss: 0.085||  Validation Loss: 0.092|| Validation Accuracy: 97.242%\n",
      "\tValidation loss decreased (0.100182 --> 0.091555).  Saving model ...\n",
      "\n",
      "Epoch: 4/10\n",
      "Training Loss: 0.066||  Validation Loss: 0.088|| Validation Accuracy: 97.292%\n",
      "\tValidation loss decreased (0.091555 --> 0.088467).  Saving model ...\n",
      "\n",
      "Epoch: 5/10\n",
      "Training Loss: 0.054||  Validation Loss: 0.080|| Validation Accuracy: 97.583%\n",
      "\tValidation loss decreased (0.088467 --> 0.080311).  Saving model ...\n",
      "\n",
      "Epoch: 6/10\n",
      "Training Loss: 0.043||  Validation Loss: 0.077|| Validation Accuracy: 97.558%\n",
      "\tValidation loss decreased (0.080311 --> 0.077146).  Saving model ...\n",
      "\n",
      "Epoch: 7/10\n",
      "Training Loss: 0.038||  Validation Loss: 0.076|| Validation Accuracy: 97.750%\n",
      "\tValidation loss decreased (0.077146 --> 0.075583).  Saving model ...\n",
      "\n",
      "Epoch: 8/10\n",
      "Training Loss: 0.031||  Validation Loss: 0.075|| Validation Accuracy: 97.758%\n",
      "\tValidation loss decreased (0.075583 --> 0.075153).  Saving model ...\n",
      "\n",
      "Epoch: 9/10\n",
      "Training Loss: 0.025||  Validation Loss: 0.077|| Validation Accuracy: 97.783%\n",
      "Epoch: 10/10\n",
      "Training Loss: 0.021||  Validation Loss: 0.072|| Validation Accuracy: 97.942%\n",
      "\tValidation loss decreased (0.075153 --> 0.071888).  Saving model ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_name = 'model.pt'\n",
    "model, train_losses, valid_losses = train(model, criterion, optimizer, epoch=10, save_file_name=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
